# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fgZaXP9_NeTyLQ-PsSxpUead3Pc_6fIq
"""

#!wget https://zenodo.org/record/841984/files/wili-2018.zip
#!unzip -u wili-2018.zip
#!pip install -U -q PyDrive
#!pip install torch torchvision

import os
from collections import Counter, defaultdict
import string
import numpy as np
import re
import pandas as pd
import torch
import torch.nn as nn
import random
import math
import pickle
from scipy import sparse
from scipy.sparse import coo_matrix, hstack, csr_matrix
from sklearn import metrics

"""#Training"""


class MLP(nn.Module):
  """
  This class implements a Multi-layer Perceptron in PyTorch.
  It handles the different layers and parameters of the model.
  Once initialized an MLP object can perform forward.
  """

  def __init__(self, n_inputs, n_hidden, n_classes):
    super(MLP, self).__init__()
    self.modules = []
    # if list is empty -> no linear layer
    prev_size = n_inputs
    if len(n_hidden) > 0:
        for n in range(len(n_hidden)):
            self.modules.append(nn.Linear(prev_size, n_hidden[n]))
            prev_size = n_hidden[n]
    self.modules.append(nn.Linear(prev_size, n_classes))

    self.relu = nn.ReLU()
    self.layers = nn.ModuleList(self.modules)

  def forward(self, x):
    out = x
    for i,m in enumerate(self.layers):
        # if not last layer
        if i == len(self.layers)-1:
            out = m(out)
        elif i != len(self.layers):
            out = m(out)
            out = self.relu(out)
    return out
  
  
  
# https://medium.com/python-learning-notes-those-cool-stuff/mini-batch-gradient-descent-ac015a8e4acc
def random_mini_batches(X, Y, mini_batch_size, seed = 0):
    """
    Creates a list of random minibatches from (X, Y)
    
    Arguments:
    X -- input data, of shape (input size, number of examples)
    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)
    mini_batch_size -- size of the mini-batches, integer
    
    Returns:
    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)
    """
    
    np.random.seed(seed)            # To make your "random" minibatches the same as ours
    m = X.shape[0]                  # number of training examples
    mini_batches = []
        
    # Step 1: Shuffle (X, Y)
    permutation = list(np.random.permutation(m))
    shuffled_X = X[permutation,:]
    shuffled_Y = Y[permutation,:].reshape((m,-1))

    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.
    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning
    for k in range(0, num_complete_minibatches):
        mini_batch_X = shuffled_X[k*mini_batch_size : (k+1)*mini_batch_size,:]
        mini_batch_Y = shuffled_Y[k*mini_batch_size : (k+1)*mini_batch_size,:]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
    
    # Handling the end case (last mini-batch < mini_batch_size)
    if m % mini_batch_size != 0:
        mini_batch_X = shuffled_X[num_complete_minibatches*mini_batch_size : m,:]
        mini_batch_Y = shuffled_Y[num_complete_minibatches*mini_batch_size : m,:]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
    
    return mini_batches
  
  
  
def accuracy(predictions, targets):
  accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(targets, axis=1))/targets.shape[0]
  return accuracy

X_train, y_train = pickle.load(open('train.p', 'rb'))
X_test, y_test = pickle.load(open('test.p', 'rb'))

X_train = X_train.todense()
X_test = X_test.todense()
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

minibatch_size = 32
epochs = 8
# Device configuration
use_cuda = torch.cuda.is_available()
if use_cuda:
    print('Running in GPU model')

device = torch.device('cuda' if use_cuda else 'cpu')
dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor


model = MLP(X_train.shape[1], [512], y_train.shape[1]).to(device)

# intialize loss function
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())
accuracies = []
eval_losses = []
losses = []
for e in range(epochs):
  total_loss = 0
  batches = random_mini_batches(X_train, y_train, minibatch_size)
  for X_train_b, y_train_b in batches:
      model.train()
      # Forward pass
      y_pred = model(torch.from_numpy(X_train_b).type(dtype))
      y_train_b = torch.from_numpy(y_train_b).type(dtype)
      t = torch.max(y_train_b,1)[1]
      loss = criterion(y_pred, t)
      total_loss += loss.item()
      # Backward and optimize
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
  # evaluate
  model.eval()
  # Forward pass
  batches = random_mini_batches(X_test, y_test, minibatch_size)
  total_eval_batch_loss = 0
  total_acc = 0
  for X_test_b, y_test_b in batches:
    # Forward pass
    y_pred = model(torch.from_numpy(X_test_b).type(dtype))
    y_test_b = torch.from_numpy(y_test_b).type(dtype)
    t = torch.max(y_test_b,1)[1]
    loss = criterion(y_pred, t)
    total_eval_batch_loss += loss.item()
    acc = metrics.accuracy_score(np.argmax(y_pred.cpu().detach().numpy(),axis=1),np.argmax(y_test_b.cpu().detach().numpy(), axis=1))
    total_acc += acc
  av_acc = total_acc/len(batches)
  av_eval_batch_loss = total_eval_batch_loss/len(batches)
  av_loss = total_loss/len(batches)
  accuracies.append(av_acc)
  eval_losses.append(av_eval_batch_loss)
  losses.append(av_loss)
  print('epoch: {}'.format(e))
  print(minibatch_size)
  print('accuracy: {}'.format(av_acc))
  print('av val batch loss: {}'.format(av_eval_batch_loss))
  print('av batch loss: {}'.format(av_loss))
  print()

pickle.dump(accuracies, open('acc.p', 'wb'))
pickle.dump(eval_losses, open('eval_loss.p', 'wb'))
pickle.dump(losses, open('loss.p', 'wb'))
torch.save(model.state_dict(), 'model.pth')
