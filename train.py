# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fgZaXP9_NeTyLQ-PsSxpUead3Pc_6fIq
"""

!wget https://zenodo.org/record/841984/files/wili-2018.zip
!unzip -u wili-2018.zip
!pip install -U -q PyDrive
!pip install torch torchvision

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
from google.colab import files
import os

# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# choose a local (colab) directory to store the data.
local_download_path = os.path.expanduser('~/data')
try:
  os.makedirs(local_download_path)
except: pass

def load_from_drive(filename):
#   flag variable 
  file_found = False
  
  # connect to drive
  file_list = drive.ListFile({'q': "'1w61UDMeEbEbnIsAmL1-kd40yvpqUSPM8' in parents"}).GetList() 

  # iterate over files
  for file in file_list:
    if file['title'] == filename:
  #       retrieve file id
      file_id = file['id']
  #       download file
      data = drive.CreateFile({'id': file_id})
      content = data.GetContentFile(filename)
      file_found = True
      break

  # if the file was found then return it
  if file_found:
    return content
  # else return False
  else:
    print('File not found on the drive !!!')
    return False

from collections import Counter, defaultdict
import string
import numpy as np
import re
import pandas as pd
import torch
import torch.nn as nn
import random
import math
import pickle
from google.colab import files
from scipy import sparse
from scipy.sparse import coo_matrix, hstack, csr_matrix

class AllLangs:
    def __init__(self, ref_lang=None):
      self.use_ref_lang = False
      if ref_lang is None:
        self.char2index = {}
        self.char2count = defaultdict(lambda: 0)
        self.index2char = defaultdict(lambda: -1)
        self.lang2index = {}
        self.index2lang = {}
        self.n_chars = 0 
        self.n_langs = 0
      else:
        self.use_ref_lang = True
        self.char2index = ref_lang.char2index
        self.char2count = ref_lang.char2count
        self.index2char = ref_lang.index2char
        self.lang2index = ref_lang.lang2index
        self.index2lang = ref_lang.index2lang
        self.n_chars = ref_lang.n_chars 
        self.n_langs = ref_lang.n_langs
        # reset counts
        self.char2count =  { x:0 for x in self.char2count}
        
    def addChars(self, chars):
      for char in chars:
        self.addChar(char)
          
    def addChar(self, char):
        if char not in self.char2count and not self.use_ref_lang:
          self.char2index[char] = self.n_chars
          self.char2count[char] = 1
          self.index2char[self.n_chars] = char
          self.n_chars += 1
        else:
          try:
            self.char2count[char] += 1
          except: pass
          
    def addLang(self, lang):
        if lang not in self.lang2index:
          self.lang2index[lang] = self.n_langs
          self.index2lang[self.n_langs] = lang
          self.n_langs += 1
          
    def one_hot(self, lang):
      one_hot_encoded = np.zeros(self.n_langs)
      one_hot_encoded[self.lang2index[lang]] = 1
      return one_hot_encoded
    
    def paragraph_tfidf(self, para_counts, lang_ref):
      lang = Lang(self, lang_ref)
      lang.addCounts(para_counts)
      return lang.to_tfidf()   
    
    def filter_paragraph_and_count(self, paragraph):
      # construct filtered counter
      c = self.char2count.copy()
      c =  { x:0 for x in c}
      for char in paragraph:
        if char in self.char2count:
          c[char] += 1
      return c
    
    def to_tfidf(self, count_para):
      def division_by_zero(x, y):
        return 0 if y == 0 else x / y
      return [division_by_zero(count_para[self.index2char[i]],self.char2count[self.index2char[i]]) for i in np.arange(len(self.char2count))]

def read_files(path_x, path_y):
  # input:
  #  path_x: (str) path of x
  #  path_y: (str) path of y
  # return:
  #   dict: {language: [paragraph, ... ,paragraph], language: [...}
  with open(path_x, 'r') as f:
    paragraphs = f.read().split('\n')
    
  with open(path_y, 'r') as f:
    languages = f.read().split('\n')
    
  data = defaultdict(lambda: [])
  for lang, para in zip(languages, paragraphs):
    para.replace('.', '')
    para = re.sub(' +', '',para)
    data[lang].append(list(para))   
  return data


def read_files_n(path_x, path_y, n):
  # input:
  #  path_x: (str) path of x
  #  path_y: (str) path of y
  # return:
  #   dict: {language: [paragraph, ... ,paragraph], language: [...}
  with open(path_x, 'r') as f:
    paragraphs = f.read().split('\n')
    
  with open(path_y, 'r') as f:
    languages = f.read().split('\n')
    
  data = defaultdict(lambda: [])
  count = 0
  for lang, para in zip(languages, paragraphs):
    count+= 1
    if count == n:
      break
    para.replace('.', '')
    para = re.sub(' +', '',para)
    data[lang].append(list(para))   
  
  return data


def preprocess(filename, lang_dict, ref_lang=None):
  # input:
  #   lang_dict: (dict) {language: [paragraph, ... ,paragraph], language: [...}
  # return:
  f = open('X_'+filename,'w')
  f2 = open('y_'+filename,'w')
  if ref_lang is None:
    all_langs = AllLangs()
    print('collecting chars')
    # accumulate all characters
    counter_all_chars = Counter([char for _,paragraphs in lang_dict.items() for para in paragraphs for char in para])
    # get characters that occure < 100
    print('filtering')
    most_common = [k for k, v in counter_all_chars.items() if v >= 100]
    print('adding filtered chars')
    # add characters to the language object
    all_langs.addChars([char for _,paragraphs in lang_dict.items() for para in paragraphs for char in para if char in most_common])
    del most_common
    print('done')
    for lang in lang_dict.keys():
      all_langs.addLang(lang)
  else:
    all_langs = AllLangs(ref_lang)
    print('collecting and filtering chars')
    # accumulate all characters and add to language if they use characters of ref lang
    all_langs.addChars([char for _,paragraphs in lang_dict.items() for para in paragraphs for char in para if char in all_langs.char2count])
  print('encoding paragraphs')
  for lang_name, paragraphs in lang_dict.items():
    # construct filtered lang_dict
    for para in paragraphs:
      para_counts = all_langs.filter_paragraph_and_count(para)
      # get tfidf of paragraph with respect to the language
      para_tfidf = all_langs.to_tfidf(para_counts)
      # get language as one hot enoded
      lang_one_hot = all_langs.one_hot(lang_name)
      f.write(','.join(map(str, para_tfidf)) + '\n')
      f2.write(','.join(map(str, lang_one_hot.astype(int))) + '\n')
  f.close()
  return all_langs

train_dict = read_files('x_train.txt', 'y_train.txt')

!rm X_train.csv
!rm y_train.csv
train_langs = preprocess('train.csv', train_dict)

del train_dict
!rm X_test.csv
!rm y_test.csv
test_dict = read_files('x_test.txt', 'y_test.txt')
test_langs = preprocess('test.csv', test_dict, train_langs)

#download files
!zip train.zip X_train.csv y_train.csv
files.download('train.zip')
!zip test.zip X_test.csv y_test.csv
files.download('test.zip')

"""#Training"""

from collections import Counter, defaultdict
import string
import numpy as np
import re
import torch
import torch.nn as nn
import random
import math
import pickle
import pandas as pd
from google.colab import files

class MLP(nn.Module):
  """
  This class implements a Multi-layer Perceptron in PyTorch.
  It handles the different layers and parameters of the model.
  Once initialized an MLP object can perform forward.
  """

  def __init__(self, n_inputs, n_hidden, n_classes):
    super(MLP, self).__init__()
    self.modules = []
    # if list is empty -> no linear layer
    prev_size = n_inputs
    if len(n_hidden) > 0:
        for n in range(len(n_hidden)):
            self.modules.append(nn.Linear(prev_size, n_hidden[n]))
            prev_size = n_hidden[n]
    self.modules.append(nn.Linear(prev_size, n_classes))

    self.relu = nn.ReLU()
    self.layers = nn.ModuleList(self.modules)

  def forward(self, x):
    out = x
    for i,m in enumerate(self.layers):
        # if not last layer
        if i == len(self.layers)-1:
            out = m(out)
        elif i != len(self.layers):
            out = m(out)
            out = self.relu(out)
    return out
  
  
  
# https://medium.com/python-learning-notes-those-cool-stuff/mini-batch-gradient-descent-ac015a8e4acc
def random_mini_batches(X, Y, mini_batch_size, seed = 0):
    """
    Creates a list of random minibatches from (X, Y)
    
    Arguments:
    X -- input data, of shape (input size, number of examples)
    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)
    mini_batch_size -- size of the mini-batches, integer
    
    Returns:
    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)
    """
    
    np.random.seed(seed)            # To make your "random" minibatches the same as ours
    m = X.shape[0]                  # number of training examples
    mini_batches = []
        
    # Step 1: Shuffle (X, Y)
    permutation = list(np.random.permutation(m))
    shuffled_X = X[permutation,:]
    shuffled_Y = Y[permutation,:].reshape((m,-1))

    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.
    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning
    for k in range(0, num_complete_minibatches):
        mini_batch_X = shuffled_X[k*mini_batch_size : (k+1)*mini_batch_size,:]
        mini_batch_Y = shuffled_Y[k*mini_batch_size : (k+1)*mini_batch_size,:]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
    
    # Handling the end case (last mini-batch < mini_batch_size)
    if m % mini_batch_size != 0:
        mini_batch_X = shuffled_X[num_complete_minibatches*mini_batch_size : m,:]
        mini_batch_Y = shuffled_Y[num_complete_minibatches*mini_batch_size : m,:]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
    
    return mini_batches
  
  
  
def accuracy(predictions, targets):
  accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(targets, axis=1))/targets.shape[0]
  return accuracy

X_train, y_train = pd.read_csv('X_train.csv', header=None),pd.read_csv('y_train.csv', header=None)
X_test, y_test = pd.read_csv('X_test.csv', header=None),pd.read_csv('y_test.csv', header=None)

X_train, y_train = X_train.values, y_train.values
X_test, y_test = X_test.values, y_test.values

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

minibatch_size = 32
epochs = 20
# Device configuration
use_cuda = torch.cuda.is_available()
if use_cuda:
    print('Running in GPU model')

device = torch.device('cuda' if use_cuda else 'cpu')
dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor


model = MLP(X_train.shape[1], [512], y_train.shape[1]).to(device)

# intialize loss function
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())
for e in range(epochs):
  total_loss = 0
  batches = random_mini_batches(X_train, y_train, minibatch_size)
  for X_train_b, y_train_b in batches:
      model.train()
      # Forward pass
      y_pred = model(torch.from_numpy(X_train_b).type(dtype))
      y_train_b = torch.from_numpy(y_train_b).type(dtype)
      t = torch.max(y_train_b,1)[1]
      loss = criterion(y_pred, t)
      total_loss += loss.item()
      # Backward and optimize
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
  # evaluate
  model.eval()
  # Forward pass
  batches = random_mini_batches(X_test, y_test, minibatch_size)
  total_eval_batch_loss = 0
  total_acc = 0
  for X_test_b, y_test_b in batches:
    # Forward pass
    y_pred = model(torch.from_numpy(X_test_b).type(dtype))
    y_test_b = torch.from_numpy(y_test_b).type(dtype)
    t = torch.max(y_test_b,1)[1]
    loss = criterion(y_pred, t)
    total_eval_batch_loss += loss.item()
    acc = accuracy(y_pred.cpu().detach().numpy(),y_test_b.cpu().detach().numpy())
    total_acc += acc
  print('epoch: {}'.format(e))
  print(minibatch_size)
  print('accuracy: {}'.format(total_acc/len(batches)))
  print('av val batch loss: {}'.format(total_eval_batch_loss/len(batches)))
  print('av batch loss: {}'.format(total_loss/len(batches)))
  print()



files.download('test.zip')

!vim y_train.txt

files.upload()

train_dict = {'ger': ['dddddkkkkk', 'sssskkkkkz'], 'ij': ['---'], 'DF': ['dddddd', 'ss']}
test_dict = {'ger': ['ddsssss','ssssssss-s'], 'ij': ['dd', 'dd']}

